{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grapycal_audio_torch.dataset import PianoRollDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset segment_len = 128\n",
      "Created dataset with 64606 data points from 2570 pieces\n"
     ]
    }
   ],
   "source": [
    "pr_ds = PianoRollDataset('../dev_cwd/_data/gr_resource/download/music/pop_piano', segment_len=128, hop_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grapycal_audio.pianoroll import PianoRoll\n",
    "\n",
    "def tokenize(pr:PianoRoll, n_velocity=128, duration:int|None=None, seq_len:int|None=None):\n",
    "    tokens = []\n",
    "    frame = 0\n",
    "    if duration is None:\n",
    "        duration = pr.duration\n",
    "\n",
    "    tokens.append({'type':'start', 'frame':frame})\n",
    "    for note in pr.notes:\n",
    "        while note.onset > frame:\n",
    "            tokens.append({'type':'next_frame', 'frame':frame})\n",
    "            frame += 1\n",
    "        \n",
    "        tokens.append({'type':'pitch', 'frame':frame, 'value':note.pitch-21})\n",
    "        tokens.append({'type':'velocity', 'frame':frame, 'value':int(note.velocity*(n_velocity/128))})\n",
    "\n",
    "    while duration > frame:\n",
    "        tokens.append({'type':'next_frame', 'frame':frame})\n",
    "        frame += 1\n",
    "\n",
    "    # fill in the next_frame\n",
    "    for i in range(len(tokens)-1):\n",
    "        tokens[i]['next_frame'] = tokens[i+1]['frame']\n",
    "\n",
    "    tokens.pop() # remove the last next_frame 128\n",
    "\n",
    "    # we're using seq_len+1 because start token doesn't count\n",
    "    if seq_len is not None:\n",
    "        tokens = tokens[:seq_len+1]\n",
    "\n",
    "        if len(tokens) < seq_len+1:\n",
    "            tokens += [{'type':'pad'}] * (seq_len+1 - len(tokens))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_positional_encoding(length:int, dim:int):\n",
    "    res = []\n",
    "    for i in range(length):\n",
    "        res.append([int(x) for x in f\"{i:0{dim}b}\"])\n",
    "        # pad\n",
    "        res[-1] += [0] * (dim - len(res[-1])) \n",
    "\n",
    "    return (\n",
    "        torch.tensor(res, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "def sinusoidal_positional_encoding(length:int, dim:int):\n",
    "    res = []\n",
    "    for d in range(dim // 2):\n",
    "        res.append(torch.sin(torch.arange(length) / 10000 ** (2 * d / dim)))\n",
    "    for d in range(dim // 2):\n",
    "        res.append(torch.cos(torch.arange(length) / 10000 ** (2 * d / dim)))\n",
    "    return torch.stack(res, dim=1)\n",
    "\n",
    "def construct_input_frame(token:dict, pos_encoding:torch.Tensor, n_pitch, n_velocity):\n",
    "\n",
    "    if token['type'] == 'pad':\n",
    "        return torch.zeros(n_pitch + n_velocity + 2 + pos_encoding.shape[1] * 2 )\n",
    "\n",
    "    #pitch\n",
    "    pitch = torch.zeros(n_pitch)\n",
    "    if token['type'] == 'pitch':\n",
    "        pitch[token['value']] = 1\n",
    "\n",
    "    #velocity\n",
    "    velocity = torch.zeros(n_velocity)\n",
    "    if token['type'] == 'velocity':\n",
    "        velocity[token['value']] = 1\n",
    "\n",
    "    #next_frame\n",
    "    next_frame = torch.zeros(1)\n",
    "    if token['type'] == 'next_frame':\n",
    "        next_frame[0] = 1\n",
    "    \n",
    "    #start\n",
    "    start = torch.zeros(1)\n",
    "    if token['type'] == 'start':\n",
    "        start[0] = 1\n",
    "\n",
    "    #pos\n",
    "    pos = pos_encoding[token['frame']]\n",
    "\n",
    "    #target pos\n",
    "    target_pos = pos_encoding[token['next_frame']]\n",
    "    \n",
    "\n",
    "    return torch.cat([pitch, velocity, next_frame, start, pos, target_pos], dim=0)\n",
    "\n",
    "def construct_input_tensor(tokens, pos_encoding:torch.Tensor, n_pitch, n_velocity):\n",
    "\n",
    "    frame_axis = []\n",
    "\n",
    "    for token in tokens:\n",
    "        frame_axis.append(construct_input_frame(token, pos_encoding, n_pitch, n_velocity))\n",
    "\n",
    "    return torch.stack(frame_axis, dim=0)\n",
    "\n",
    "def construct_output_mask(tokens, n_pitch, n_velocity):\n",
    "    '''\n",
    "    An additive mask for the model's output (logits) to prevent the model from predicting invalid tokens.\n",
    "\n",
    "    The first token must be pitch or next_frame.\n",
    "    The next token of pitch must be velocity.\n",
    "    The next token of next_frame can be pitch or next_frame.\n",
    "    The next token of velocity must be pitch or next_frame.\n",
    "\n",
    "    Accroding to the above rule, we can construct a mask as a prior on the model's prediction.\n",
    "    '''\n",
    "\n",
    "    mask = torch.zeros(len(tokens), n_pitch + n_velocity + 1)\n",
    "    # fill with -inf\n",
    "    mask = mask - 1e7\n",
    "\n",
    "    mask[0, :n_pitch] = 0\n",
    "    mask[0, n_pitch+n_velocity] = 0\n",
    "\n",
    "    for i in range(len(tokens)-1):\n",
    "        # output shape: Output: [pitch(n_pitch), velocity(n_velocity), next_frame(1)]\n",
    "        token = tokens[i]\n",
    "\n",
    "        if token['type'] == 'pitch':\n",
    "            #enable velocity\n",
    "            mask[i+1, n_pitch:n_pitch+n_velocity] = 0\n",
    "        if token['type'] == 'velocity':\n",
    "            #enable pitch or next_frame\n",
    "            mask[i+1, :n_pitch] = 0\n",
    "            mask[i+1, n_pitch+n_velocity] = 0\n",
    "        if token['type'] == 'next_frame':\n",
    "            #enable pitch or next_frame\n",
    "            mask[i+1, :n_pitch] = 0\n",
    "            mask[i+1, n_pitch+n_velocity] = 0\n",
    "\n",
    "    return mask\n",
    "\n",
    "def construct_target(tokens, n_pitch, n_velocity):\n",
    "    res = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token['type'] == 'pitch':\n",
    "            res.append(token['value'])\n",
    "        elif token['type'] == 'velocity':\n",
    "            res.append(n_pitch + token['value'])\n",
    "        elif token['type'] == 'next_frame':\n",
    "            res.append(n_pitch + n_velocity)\n",
    "        elif token['type'] == 'pad':\n",
    "            res.append(-100) # -100 is the ignore index\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown token type: {token['type']}\")\n",
    "\n",
    "    return torch.tensor(res, dtype=torch.long)\n",
    "\n",
    "pos_encoding = torch.cat([\n",
    "    binary_positional_encoding(128,8),\n",
    "    sinusoidal_positional_encoding(128, 32)\n",
    "], dim=1)\n",
    "\n",
    "class TokenizedPianoRollDataset(Dataset):\n",
    "    '''\n",
    "    Input: [pitch(n_pitch), velocity(n_velocity), next_frame(1), start(1), pos, target_pos]\n",
    "    Output: [pitch(n_pitch), velocity(n_velocity), next_frame(1)]\n",
    "    '''\n",
    "    def __init__(self, ds:PianoRollDataset, pos_encoding:torch.Tensor, segment_length:int, seq_len:int, n_pitch:int, n_velocity:int):\n",
    "        self.ds = ds\n",
    "        self.pos_encoding = pos_encoding\n",
    "        self.seq_len = seq_len\n",
    "        self.n_pitch = n_pitch\n",
    "        self.n_velocity = n_velocity\n",
    "        self.segment_length = segment_length\n",
    "\n",
    "        self.tokens = []\n",
    "        for idx in range(len(self.ds)):\n",
    "            self.tokens.append(tokenize(self.ds.get_piano_roll(idx), n_velocity=self.n_velocity, duration=self.segment_length, seq_len=self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens[idx]\n",
    "        tokens_without_start = tokens[1:]\n",
    "\n",
    "        # the last token is not needed to be an input\n",
    "        input = construct_input_tensor(tokens[:-1], pos_encoding=self.pos_encoding, n_pitch=self.n_pitch, n_velocity=self.n_velocity)\n",
    "        target = construct_target(tokens_without_start, n_pitch=self.n_pitch, n_velocity=self.n_velocity)\n",
    "        output_mask = construct_output_mask(tokens_without_start, n_pitch=self.n_pitch, n_velocity=self.n_velocity)\n",
    "        return {'input':input, 'target':target, 'output_mask':output_mask}\n",
    "    \n",
    "    def get_loss_weight(self):\n",
    "        '''\n",
    "        The loss weight for each token.\n",
    "        '''\n",
    "        res = torch.ones(self.n_pitch + self.n_velocity + 1)\n",
    "        res[self.n_pitch + self.n_velocity] = 0.05 # next_frame is too common so we need to reduce its weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TokenizedPianoRollDataset(pr_ds, pos_encoding, 128, 350, 88, 32)\n",
    "dl = DataLoader(ds, batch_size=24, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "# input: B, 350, 202\n",
    "# output: B, 350, 121\n",
    "\n",
    "class PianoRollGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_linear = nn.Linear(202, 256)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=1024, batch_first=True), num_layers=6)\n",
    "        self.out_linear = nn.Linear(256, 121)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_linear(x)\n",
    "        x = self.transformer(x, mask = nn.Transformer.generate_square_subsequent_mask(x.shape[1]).to(x.device), is_causal = True)\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "        \n",
    "model = PianoRollGenerator()\n",
    "\n",
    "crit = nn.CrossEntropyLoss(weight=ds.get_loss_weight())\n",
    "\n",
    "opt = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from grapycal_audio.pianoroll import Note\n",
    "\n",
    "def top_k(logits:torch.Tensor, k):\n",
    "    values, indices = logits.topk(k)\n",
    "    probs = torch.softmax(values, dim=0)\n",
    "    selected = torch.multinomial(probs, 1)\n",
    "    return indices[selected]\n",
    "\n",
    "def decode(logits, last_token, n_pitch, n_velocity):\n",
    "    frame = last_token['next_frame']\n",
    "\n",
    "    if last_token['type'] in ['start', 'velocity', 'next_frame']:\n",
    "        logits[n_pitch:n_pitch+n_velocity] = - torch.inf\n",
    "        max_idx = top_k(logits, 15).item()\n",
    "        if max_idx < n_pitch:\n",
    "            return {'type':'pitch', 'value':max_idx, 'frame':frame, 'next_frame':frame}\n",
    "        elif max_idx == n_pitch + n_velocity:\n",
    "            return {'type':'next_frame', 'frame':frame, 'next_frame':frame+1}\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid index: {max_idx}\")\n",
    "        \n",
    "    elif last_token['type'] == 'pitch':\n",
    "        logits[:n_pitch] = -torch.inf\n",
    "        logits[n_pitch+n_velocity] = -torch.inf\n",
    "        max_idx = top_k(logits, 15).item()\n",
    "        return {'type':'velocity', 'value':max_idx - n_pitch, 'frame':frame, 'next_frame':frame}\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown token type: {last_token['type']}\")\n",
    "    \n",
    "def token_to_pianoroll(tokens):\n",
    "    notes = []\n",
    "    frame = 0\n",
    "    last_pitch = None\n",
    "    for token in tokens:\n",
    "        if token['type'] == 'start':\n",
    "            continue\n",
    "        if token['type'] == 'pitch':\n",
    "            last_pitch = token['value']\n",
    "        if token['type'] == 'velocity':\n",
    "            notes.append(Note(onset=frame, pitch=last_pitch+21, velocity=int(token['value']*(128/32))))\n",
    "        if token['type'] == 'next_frame':\n",
    "            frame += 1\n",
    "    return PianoRoll(notes)\n",
    "    \n",
    "# logits = out[0].detach().cpu()\n",
    "\n",
    "# n_pitch = 88\n",
    "# n_velocity = 32\n",
    "# last_token = {'type':'start', 'frame':0, 'next_frame':0}\n",
    "# tokens = []\n",
    "# for frame_logits in logits:\n",
    "#     decoded = decode(frame_logits, last_token, n_pitch, n_velocity)\n",
    "#     tokens.append(decoded)\n",
    "#     last_token = decoded\n",
    "def inference(file_path:str):\n",
    "    model.eval()\n",
    "    n_pitch = 88\n",
    "    n_velocity = 32\n",
    "    tokens = [{'type':'start', 'frame':0, 'next_frame':0}]\n",
    "    #tokens = ds.tokens[64][:20]\n",
    "    last_token = tokens[-1]\n",
    "    while tokens[-1]['next_frame'] < 128:\n",
    "        input = construct_input_tensor(tokens, pos_encoding=pos_encoding, n_pitch=n_pitch, n_velocity=n_velocity).unsqueeze(0)\n",
    "        input = input.to(device)\n",
    "        logits = model(input).squeeze(0)[-1].detach().cpu()\n",
    "        decoded = decode(logits, last_token, n_pitch, n_velocity)\n",
    "        tokens.append(decoded)\n",
    "        last_token = decoded\n",
    "\n",
    "    token_to_pianoroll(tokens).to_midi(file_path)\n",
    "\n",
    "inference(str(random.randint(0, 1000))+'.mid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2692/2692 [01:24<00:00, 31.98it/s, batch=2600, gpu_temp=58, loss=1.04] \n",
      " 23%|██▎       | 622/2692 [00:24<01:20, 25.56it/s, batch=600, gpu_temp=57, loss=1.02] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# save a midi file\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         inference(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss is NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "crit.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    tq = tqdm(dl)\n",
    "    for i, batch in enumerate(tq):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        opt.zero_grad()\n",
    "        out = model(batch['input'])\n",
    "        loss = crit((out+batch['output_mask']).transpose(1,2), batch['target'])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if i % 100 == 0:\n",
    "            # print the loss to tqdm\n",
    "            temp = torch.cuda.temperature()\n",
    "            tq.set_postfix(batch = i, loss= loss.item(), gpu_temp=temp)\n",
    "\n",
    "            if temp > 65:\n",
    "                print(\"GPU temperature is too high. Slowin down.\", temp)\n",
    "                time.sleep(0.1)\n",
    "        if i%500 == 0:\n",
    "            # save a midi file\n",
    "            inference(f'./output_{epoch}_{i}.mid')\n",
    "                    \n",
    "        if torch.isnan(loss):\n",
    "            raise ValueError(\"Loss is NaN\")\n",
    "    torch.save(model.state_dict(), f'./model_{epoch}.pth')\n",
    "    torch.save(opt.state_dict(), f'./opt_{epoch}.pth')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4156, -4.4891, -3.1106, -3.2990, -3.3759, -1.5643, -0.1575, -0.3899,\n",
       "        -0.7835, -0.1276,  0.0869, -2.8013,  1.5738, -1.6056,  0.7026, -0.2029,\n",
       "        -1.9337,  0.6056,  0.2674,  0.7529,  0.2170, -0.0785,  0.3777, -2.3267,\n",
       "        -0.8005, -0.8463,  0.5303, -2.2141, -2.2994, -1.1026, -5.5723, -2.1356,\n",
       "        -1.8027, -2.7682, -2.8734, -2.2773,  1.0160, -2.0810, -2.1232,  0.5919,\n",
       "        -0.1786, -0.1194, -1.0934,  0.7538, -0.3089, -0.4101,  2.0250,  2.1511,\n",
       "         2.9140, -1.5608,  1.4655,  1.6449, -0.8841, -1.5438, -3.4564, -3.3379,\n",
       "         0.3282, -3.0211, -3.8730, -0.9413,  0.6791, -7.3599,  0.2818, -2.5658,\n",
       "        -2.7531, -3.6532, -3.0842, -1.4258, -1.8834, -2.5331, -3.3839, -1.6692,\n",
       "        -0.6860, -3.0004, -1.5400, -3.2789, -2.7134, -1.2236, -1.6472, -0.5175,\n",
       "         0.8821,  0.0575, -0.2140, -1.7927,  1.0660, -0.7166,  0.2254,  0.0662,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "         8.4794])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'start', 'frame': 0, 'next_frame': 0},\n",
       " {'type': 'pitch', 'frame': 0, 'value': 48, 'next_frame': 0},\n",
       " {'type': 'velocity', 'frame': 0, 'value': 17, 'next_frame': 0},\n",
       " {'type': 'next_frame', 'frame': 0, 'next_frame': 1},\n",
       " {'type': 'next_frame', 'frame': 1, 'next_frame': 2},\n",
       " {'type': 'next_frame', 'frame': 2, 'next_frame': 3},\n",
       " {'type': 'next_frame', 'frame': 3, 'next_frame': 4},\n",
       " {'type': 'next_frame', 'frame': 4, 'next_frame': 5},\n",
       " {'type': 'pitch', 'frame': 5, 'value': 12, 'next_frame': 5},\n",
       " {'type': 'velocity', 'frame': 5, 'value': 15, 'next_frame': 5},\n",
       " {'type': 'next_frame', 'frame': 5, 'next_frame': 6},\n",
       " {'type': 'next_frame', 'frame': 6, 'next_frame': 7},\n",
       " {'type': 'next_frame', 'frame': 7, 'next_frame': 8},\n",
       " {'type': 'pitch', 'frame': 8, 'value': 39, 'next_frame': 8},\n",
       " {'type': 'velocity', 'frame': 8, 'value': 18, 'next_frame': 8},\n",
       " {'type': 'next_frame', 'frame': 8, 'next_frame': 9},\n",
       " {'type': 'pitch', 'frame': 9, 'value': 19, 'next_frame': 9},\n",
       " {'type': 'velocity', 'frame': 9, 'value': 12, 'next_frame': 9},\n",
       " {'type': 'next_frame', 'frame': 9, 'next_frame': 10},\n",
       " {'type': 'next_frame', 'frame': 10, 'next_frame': 11},\n",
       " {'type': 'next_frame', 'frame': 11, 'next_frame': 12},\n",
       " {'type': 'pitch', 'frame': 12, 'value': 24, 'next_frame': 12},\n",
       " {'type': 'velocity', 'frame': 12, 'value': 13, 'next_frame': 12},\n",
       " {'type': 'pitch', 'frame': 12, 'value': 38, 'next_frame': 12},\n",
       " {'type': 'velocity', 'frame': 12, 'value': 17, 'next_frame': 12},\n",
       " {'type': 'next_frame', 'frame': 12, 'next_frame': 13},\n",
       " {'type': 'next_frame', 'frame': 13, 'next_frame': 14},\n",
       " {'type': 'next_frame', 'frame': 14, 'next_frame': 15},\n",
       " {'type': 'next_frame', 'frame': 15, 'next_frame': 16},\n",
       " {'type': 'pitch', 'frame': 16, 'value': 19, 'next_frame': 16},\n",
       " {'type': 'velocity', 'frame': 16, 'value': 14, 'next_frame': 16},\n",
       " {'type': 'pitch', 'frame': 16, 'value': 36, 'next_frame': 16},\n",
       " {'type': 'velocity', 'frame': 16, 'value': 17, 'next_frame': 16},\n",
       " {'type': 'next_frame', 'frame': 16, 'next_frame': 17},\n",
       " {'type': 'next_frame', 'frame': 17, 'next_frame': 18},\n",
       " {'type': 'next_frame', 'frame': 18, 'next_frame': 19},\n",
       " {'type': 'next_frame', 'frame': 19, 'next_frame': 20},\n",
       " {'type': 'pitch', 'frame': 20, 'value': 24, 'next_frame': 20},\n",
       " {'type': 'velocity', 'frame': 20, 'value': 13, 'next_frame': 20},\n",
       " {'type': 'next_frame', 'frame': 20, 'next_frame': 21},\n",
       " {'type': 'next_frame', 'frame': 21, 'next_frame': 22},\n",
       " {'type': 'next_frame', 'frame': 22, 'next_frame': 23},\n",
       " {'type': 'next_frame', 'frame': 23, 'next_frame': 24},\n",
       " {'type': 'pitch', 'frame': 24, 'value': 19, 'next_frame': 24},\n",
       " {'type': 'velocity', 'frame': 24, 'value': 14, 'next_frame': 24},\n",
       " {'type': 'pitch', 'frame': 24, 'value': 39, 'next_frame': 24},\n",
       " {'type': 'velocity', 'frame': 24, 'value': 19, 'next_frame': 24},\n",
       " {'type': 'pitch', 'frame': 24, 'value': 51, 'next_frame': 24},\n",
       " {'type': 'velocity', 'frame': 24, 'value': 20, 'next_frame': 24},\n",
       " {'type': 'next_frame', 'frame': 24, 'next_frame': 25},\n",
       " {'type': 'next_frame', 'frame': 25, 'next_frame': 26},\n",
       " {'type': 'next_frame', 'frame': 26, 'next_frame': 27},\n",
       " {'type': 'next_frame', 'frame': 27, 'next_frame': 28},\n",
       " {'type': 'next_frame', 'frame': 28, 'next_frame': 29},\n",
       " {'type': 'next_frame', 'frame': 29, 'next_frame': 30},\n",
       " {'type': 'next_frame', 'frame': 30, 'next_frame': 31},\n",
       " {'type': 'next_frame', 'frame': 31, 'next_frame': 32},\n",
       " {'type': 'next_frame', 'frame': 32, 'next_frame': 33},\n",
       " {'type': 'next_frame', 'frame': 33, 'next_frame': 34},\n",
       " {'type': 'next_frame', 'frame': 34, 'next_frame': 35},\n",
       " {'type': 'next_frame', 'frame': 35, 'next_frame': 36},\n",
       " {'type': 'next_frame', 'frame': 36, 'next_frame': 37},\n",
       " {'type': 'next_frame', 'frame': 37, 'next_frame': 38},\n",
       " {'type': 'next_frame', 'frame': 38, 'next_frame': 39},\n",
       " {'type': 'next_frame', 'frame': 39, 'next_frame': 40},\n",
       " {'type': 'next_frame', 'frame': 40, 'next_frame': 41},\n",
       " {'type': 'next_frame', 'frame': 41, 'next_frame': 42},\n",
       " {'type': 'next_frame', 'frame': 42, 'next_frame': 43},\n",
       " {'type': 'next_frame', 'frame': 43, 'next_frame': 44},\n",
       " {'type': 'next_frame', 'frame': 44, 'next_frame': 45},\n",
       " {'type': 'next_frame', 'frame': 45, 'next_frame': 46},\n",
       " {'type': 'next_frame', 'frame': 46, 'next_frame': 47},\n",
       " {'type': 'next_frame', 'frame': 47, 'next_frame': 48},\n",
       " {'type': 'next_frame', 'frame': 48, 'next_frame': 49},\n",
       " {'type': 'next_frame', 'frame': 49, 'next_frame': 50},\n",
       " {'type': 'next_frame', 'frame': 50, 'next_frame': 51},\n",
       " {'type': 'next_frame', 'frame': 51, 'next_frame': 52},\n",
       " {'type': 'next_frame', 'frame': 52, 'next_frame': 53},\n",
       " {'type': 'next_frame', 'frame': 53, 'next_frame': 54},\n",
       " {'type': 'next_frame', 'frame': 54, 'next_frame': 55},\n",
       " {'type': 'next_frame', 'frame': 55, 'next_frame': 56},\n",
       " {'type': 'next_frame', 'frame': 56, 'next_frame': 57},\n",
       " {'type': 'next_frame', 'frame': 57, 'next_frame': 58},\n",
       " {'type': 'next_frame', 'frame': 58, 'next_frame': 59},\n",
       " {'type': 'next_frame', 'frame': 59, 'next_frame': 60},\n",
       " {'type': 'next_frame', 'frame': 60, 'next_frame': 61},\n",
       " {'type': 'next_frame', 'frame': 61, 'next_frame': 62},\n",
       " {'type': 'next_frame', 'frame': 62, 'next_frame': 63},\n",
       " {'type': 'next_frame', 'frame': 63, 'next_frame': 64},\n",
       " {'type': 'next_frame', 'frame': 64, 'next_frame': 65},\n",
       " {'type': 'next_frame', 'frame': 65, 'next_frame': 66},\n",
       " {'type': 'next_frame', 'frame': 66, 'next_frame': 67},\n",
       " {'type': 'next_frame', 'frame': 67, 'next_frame': 68},\n",
       " {'type': 'next_frame', 'frame': 68, 'next_frame': 69},\n",
       " {'type': 'next_frame', 'frame': 69, 'next_frame': 70},\n",
       " {'type': 'next_frame', 'frame': 70, 'next_frame': 71},\n",
       " {'type': 'next_frame', 'frame': 71, 'next_frame': 72},\n",
       " {'type': 'next_frame', 'frame': 72, 'next_frame': 73},\n",
       " {'type': 'next_frame', 'frame': 73, 'next_frame': 74},\n",
       " {'type': 'next_frame', 'frame': 74, 'next_frame': 75},\n",
       " {'type': 'next_frame', 'frame': 75, 'next_frame': 76},\n",
       " {'type': 'next_frame', 'frame': 76, 'next_frame': 77},\n",
       " {'type': 'next_frame', 'frame': 77, 'next_frame': 78},\n",
       " {'type': 'next_frame', 'frame': 78, 'next_frame': 79},\n",
       " {'type': 'next_frame', 'frame': 79, 'next_frame': 80},\n",
       " {'type': 'next_frame', 'frame': 80, 'next_frame': 81},\n",
       " {'type': 'next_frame', 'frame': 81, 'next_frame': 82},\n",
       " {'type': 'next_frame', 'frame': 82, 'next_frame': 83},\n",
       " {'type': 'next_frame', 'frame': 83, 'next_frame': 84},\n",
       " {'type': 'next_frame', 'frame': 84, 'next_frame': 85},\n",
       " {'type': 'next_frame', 'frame': 85, 'next_frame': 86},\n",
       " {'type': 'next_frame', 'frame': 86, 'next_frame': 87},\n",
       " {'type': 'next_frame', 'frame': 87, 'next_frame': 88},\n",
       " {'type': 'next_frame', 'frame': 88, 'next_frame': 89},\n",
       " {'type': 'next_frame', 'frame': 89, 'next_frame': 90},\n",
       " {'type': 'next_frame', 'frame': 90, 'next_frame': 91},\n",
       " {'type': 'next_frame', 'frame': 91, 'next_frame': 92},\n",
       " {'type': 'next_frame', 'frame': 92, 'next_frame': 93},\n",
       " {'type': 'next_frame', 'frame': 93, 'next_frame': 94},\n",
       " {'type': 'next_frame', 'frame': 94, 'next_frame': 95},\n",
       " {'type': 'next_frame', 'frame': 95, 'next_frame': 96},\n",
       " {'type': 'next_frame', 'frame': 96, 'next_frame': 97},\n",
       " {'type': 'next_frame', 'frame': 97, 'next_frame': 98},\n",
       " {'type': 'next_frame', 'frame': 98, 'next_frame': 99},\n",
       " {'type': 'next_frame', 'frame': 99, 'next_frame': 100},\n",
       " {'type': 'next_frame', 'frame': 100, 'next_frame': 101},\n",
       " {'type': 'next_frame', 'frame': 101, 'next_frame': 102},\n",
       " {'type': 'next_frame', 'frame': 102, 'next_frame': 103},\n",
       " {'type': 'next_frame', 'frame': 103, 'next_frame': 104},\n",
       " {'type': 'next_frame', 'frame': 104, 'next_frame': 105},\n",
       " {'type': 'next_frame', 'frame': 105, 'next_frame': 106},\n",
       " {'type': 'next_frame', 'frame': 106, 'next_frame': 107},\n",
       " {'type': 'pitch', 'value': 24, 'frame': 107, 'next_frame': 107},\n",
       " {'type': 'velocity', 'value': 23, 'frame': 107, 'next_frame': 107},\n",
       " {'type': 'next_frame', 'frame': 107, 'next_frame': 108},\n",
       " {'type': 'next_frame', 'frame': 108, 'next_frame': 109},\n",
       " {'type': 'next_frame', 'frame': 109, 'next_frame': 110},\n",
       " {'type': 'next_frame', 'frame': 110, 'next_frame': 111},\n",
       " {'type': 'next_frame', 'frame': 111, 'next_frame': 112},\n",
       " {'type': 'next_frame', 'frame': 112, 'next_frame': 113},\n",
       " {'type': 'next_frame', 'frame': 113, 'next_frame': 114},\n",
       " {'type': 'next_frame', 'frame': 114, 'next_frame': 115},\n",
       " {'type': 'next_frame', 'frame': 115, 'next_frame': 116},\n",
       " {'type': 'next_frame', 'frame': 116, 'next_frame': 117},\n",
       " {'type': 'next_frame', 'frame': 117, 'next_frame': 118},\n",
       " {'type': 'next_frame', 'frame': 118, 'next_frame': 119},\n",
       " {'type': 'next_frame', 'frame': 119, 'next_frame': 120},\n",
       " {'type': 'next_frame', 'frame': 120, 'next_frame': 121},\n",
       " {'type': 'next_frame', 'frame': 121, 'next_frame': 122},\n",
       " {'type': 'next_frame', 'frame': 122, 'next_frame': 123},\n",
       " {'type': 'next_frame', 'frame': 123, 'next_frame': 124},\n",
       " {'type': 'next_frame', 'frame': 124, 'next_frame': 125},\n",
       " {'type': 'next_frame', 'frame': 125, 'next_frame': 126},\n",
       " {'type': 'next_frame', 'frame': 126, 'next_frame': 127},\n",
       " {'type': 'next_frame', 'frame': 127, 'next_frame': 128}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
