{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grapycal_audio_torch.dataset import PianoRollDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grapycal_audio.pianoroll import PianoRoll\n",
    "\n",
    "def tokenize(pr:PianoRoll, n_velocity=128, duration:int|None=None, seq_len:int|None=None):\n",
    "    tokens = []\n",
    "    frame = 0\n",
    "    if duration is None:\n",
    "        duration = pr.duration\n",
    "\n",
    "    tokens.append({'type':'start', 'frame':frame})\n",
    "    for note in pr.notes:\n",
    "        while note.onset > frame:\n",
    "            tokens.append({'type':'next_frame', 'frame':frame})\n",
    "            frame += 1\n",
    "        \n",
    "        tokens.append({'type':'pitch', 'frame':frame, 'value':note.pitch-21})\n",
    "        tokens.append({'type':'velocity', 'frame':frame, 'value':int(note.velocity*(n_velocity/128))})\n",
    "\n",
    "    while duration > frame:\n",
    "        tokens.append({'type':'next_frame', 'frame':frame})\n",
    "        frame += 1\n",
    "\n",
    "    # fill in the next_frame\n",
    "    for i in range(len(tokens)-1):\n",
    "        tokens[i]['next_frame'] = tokens[i+1]['frame']\n",
    "\n",
    "    tokens.pop() # remove the last next_frame 128\n",
    "\n",
    "    # we're using seq_len+1 because start token doesn't count\n",
    "    if seq_len is not None:\n",
    "        tokens = tokens[:seq_len+1]\n",
    "\n",
    "        if len(tokens) < seq_len+1:\n",
    "            tokens += [{'type':'pad'}] * (seq_len+1 - len(tokens))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_positional_encoding(length:int, dim:int):\n",
    "    res = []\n",
    "    for i in range(length):\n",
    "        res.append([int(x) for x in f\"{i:0{dim}b}\"])\n",
    "        # pad\n",
    "        res[-1] += [0] * (dim - len(res[-1])) \n",
    "\n",
    "    return (\n",
    "        torch.tensor(res, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "def sinusoidal_positional_encoding(length:int, dim:int):\n",
    "    res = []\n",
    "    for d in range(dim // 2):\n",
    "        res.append(torch.sin(torch.arange(length) / 10000 ** (2 * d / dim)))\n",
    "    for d in range(dim // 2):\n",
    "        res.append(torch.cos(torch.arange(length) / 10000 ** (2 * d / dim)))\n",
    "    return torch.stack(res, dim=1)\n",
    "\n",
    "def construct_input_frame(token:dict, pos_encoding:torch.Tensor, n_pitch, n_velocity):\n",
    "\n",
    "    if token['type'] == 'pad':\n",
    "        return torch.zeros(n_pitch + n_velocity + 2 + pos_encoding.shape[1] * 2 )\n",
    "\n",
    "    #pitch\n",
    "    pitch = torch.zeros(n_pitch)\n",
    "    if token['type'] == 'pitch':\n",
    "        pitch[token['value']] = 1\n",
    "\n",
    "    #velocity\n",
    "    velocity = torch.zeros(n_velocity)\n",
    "    if token['type'] == 'velocity':\n",
    "        velocity[token['value']] = 1\n",
    "\n",
    "    #next_frame\n",
    "    next_frame = torch.zeros(1)\n",
    "    if token['type'] == 'next_frame':\n",
    "        next_frame[0] = 1\n",
    "    \n",
    "    #start\n",
    "    start = torch.zeros(1)\n",
    "    if token['type'] == 'start':\n",
    "        start[0] = 1\n",
    "\n",
    "    #pos\n",
    "    pos = pos_encoding[token['frame']]\n",
    "\n",
    "    #target pos\n",
    "    target_pos = pos_encoding[token['next_frame']]\n",
    "    \n",
    "\n",
    "    return torch.cat([pitch, velocity, next_frame, start, pos, target_pos], dim=0)\n",
    "\n",
    "def construct_input_tensor(tokens, pos_encoding:torch.Tensor, n_pitch, n_velocity):\n",
    "\n",
    "    frame_axis = []\n",
    "\n",
    "    for token in tokens:\n",
    "        frame_axis.append(construct_input_frame(token, pos_encoding, n_pitch, n_velocity))\n",
    "\n",
    "    return torch.stack(frame_axis, dim=0)\n",
    "\n",
    "def construct_output_mask(tokens, n_pitch, n_velocity):\n",
    "    '''\n",
    "    An additive mask for the model's output (logits) to prevent the model from predicting invalid tokens.\n",
    "\n",
    "    The first token must be pitch or next_frame.\n",
    "    The next token of pitch must be velocity.\n",
    "    The next token of next_frame can be pitch or next_frame.\n",
    "    The next token of velocity must be pitch or next_frame.\n",
    "\n",
    "    Accroding to the above rule, we can construct a mask as a prior on the model's prediction.\n",
    "    '''\n",
    "\n",
    "    mask = torch.zeros(len(tokens), n_pitch + n_velocity + 1)\n",
    "    # fill with -inf\n",
    "    mask = mask - 1e7\n",
    "\n",
    "    mask[0, :n_pitch] = 0\n",
    "    mask[0, n_pitch+n_velocity] = 0\n",
    "\n",
    "    for i in range(len(tokens)-1):\n",
    "        # output shape: Output: [pitch(n_pitch), velocity(n_velocity), next_frame(1)]\n",
    "        token = tokens[i]\n",
    "\n",
    "        if token['type'] == 'pitch':\n",
    "            #enable velocity\n",
    "            mask[i+1, n_pitch:n_pitch+n_velocity] = 0\n",
    "        if token['type'] == 'velocity':\n",
    "            #enable pitch or next_frame\n",
    "            mask[i+1, :n_pitch] = 0\n",
    "            mask[i+1, n_pitch+n_velocity] = 0\n",
    "        if token['type'] == 'next_frame':\n",
    "            #enable pitch or next_frame\n",
    "            mask[i+1, :n_pitch] = 0\n",
    "            mask[i+1, n_pitch+n_velocity] = 0\n",
    "\n",
    "    return mask\n",
    "\n",
    "def construct_target(tokens, n_pitch, n_velocity):\n",
    "    res = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token['type'] == 'pitch':\n",
    "            res.append(token['value'])\n",
    "        elif token['type'] == 'velocity':\n",
    "            res.append(n_pitch + token['value'])\n",
    "        elif token['type'] == 'next_frame':\n",
    "            res.append(n_pitch + n_velocity)\n",
    "        elif token['type'] == 'pad':\n",
    "            res.append(-100) # -100 is the ignore index\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown token type: {token['type']}\")\n",
    "\n",
    "    return torch.tensor(res, dtype=torch.long)\n",
    "\n",
    "pos_encoding = torch.cat([\n",
    "    binary_positional_encoding(512,9),\n",
    "    sinusoidal_positional_encoding(512, 31)\n",
    "], dim=1)\n",
    "\n",
    "class TokenizedPianoRollDataset(Dataset):\n",
    "    '''\n",
    "    Input: [pitch(n_pitch), velocity(n_velocity), next_frame(1), start(1), pos, target_pos]\n",
    "    Output: [pitch(n_pitch), velocity(n_velocity), next_frame(1)]\n",
    "    '''\n",
    "    def __init__(self, path:str, pos_encoding:torch.Tensor, segment_length:int, hop_len:int, seq_len:int, n_pitch:int, n_velocity:int):\n",
    "        self.ds = PianoRollDataset(path, segment_len=segment_length, hop_len=hop_len)\n",
    "        self.pos_encoding = pos_encoding\n",
    "        self.seq_len = seq_len\n",
    "        self.n_pitch = n_pitch\n",
    "        self.n_velocity = n_velocity\n",
    "        self.segment_length = segment_length\n",
    "\n",
    "        #self.tokens = []\n",
    "        #for idx in range(len(self.ds)):\n",
    "        #    self.tokens.append(tokenize(self.ds.get_piano_roll(idx), n_velocity=self.n_velocity, duration=self.segment_length, seq_len=self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #tokens = self.tokens[idx]\n",
    "\n",
    "        pr = self.ds.get_piano_roll(idx)\n",
    "        tokens = tokenize(pr, n_velocity=self.n_velocity, duration=self.segment_length, seq_len=self.seq_len)\n",
    "\n",
    "        tokens_without_start = tokens[1:]\n",
    "\n",
    "        # the last token is not needed to be an input\n",
    "        input = construct_input_tensor(tokens[:-1], pos_encoding=self.pos_encoding, n_pitch=self.n_pitch, n_velocity=self.n_velocity)\n",
    "        target = construct_target(tokens_without_start, n_pitch=self.n_pitch, n_velocity=self.n_velocity)\n",
    "        output_mask = construct_output_mask(tokens_without_start, n_pitch=self.n_pitch, n_velocity=self.n_velocity)\n",
    "        return {'input':input, 'target':target, 'output_mask':output_mask}\n",
    "    \n",
    "    def get_loss_weight(self):\n",
    "        '''\n",
    "        The loss weight for each token.\n",
    "        '''\n",
    "        res = torch.ones(self.n_pitch + self.n_velocity + 1)\n",
    "        res[self.n_pitch + self.n_velocity] = 0.05 # next_frame is too common so we need to reduce its weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TokenizedPianoRollDataset('../dev_cwd/_data/gr_resource/download/music/pop_piano', pos_encoding, 512, 512, 1400, 88, 32)\n",
    "dl = DataLoader(ds,batch_size=8, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset segment_len = 5\n",
      "Created dataset with 13 data points from 3 pieces\n"
     ]
    }
   ],
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "# input: B, 350, 202\n",
    "# output: B, 350, 121\n",
    "\n",
    "class PianoRollGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_linear = nn.Linear(200, 256)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=1024, batch_first=True), num_layers=6)\n",
    "        self.out_linear = nn.Linear(256, 121)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_linear(x)\n",
    "        x = self.transformer(x, mask = nn.Transformer.generate_square_subsequent_mask(x.shape[1]).to(x.device), is_causal = True)\n",
    "        x = self.out_linear(x)\n",
    "        return x\n",
    "        \n",
    "model = PianoRollGenerator()\n",
    "\n",
    "crit = nn.CrossEntropyLoss(weight=ds.get_loss_weight())\n",
    "\n",
    "opt = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from grapycal_audio.pianoroll import Note\n",
    "\n",
    "def top_k(logits:torch.Tensor, k):\n",
    "    values, indices = logits.topk(k)\n",
    "    probs = torch.softmax(values, dim=0)\n",
    "    selected = torch.multinomial(probs, 1)\n",
    "    return indices[selected]\n",
    "\n",
    "def decode(logits, last_token, n_pitch, n_velocity):\n",
    "    frame = last_token['next_frame']\n",
    "\n",
    "    if last_token['type'] in ['start', 'velocity', 'next_frame']:\n",
    "        logits[n_pitch:n_pitch+n_velocity] = - torch.inf\n",
    "        max_idx = top_k(logits, 15).item()\n",
    "        if max_idx < n_pitch:\n",
    "            return {'type':'pitch', 'value':max_idx, 'frame':frame, 'next_frame':frame}\n",
    "        elif max_idx == n_pitch + n_velocity:\n",
    "            return {'type':'next_frame', 'frame':frame, 'next_frame':frame+1}\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid index: {max_idx}\")\n",
    "        \n",
    "    elif last_token['type'] == 'pitch':\n",
    "        logits[:n_pitch] = -torch.inf\n",
    "        logits[n_pitch+n_velocity] = -torch.inf\n",
    "        max_idx = top_k(logits, 15).item()\n",
    "        return {'type':'velocity', 'value':max_idx - n_pitch, 'frame':frame, 'next_frame':frame}\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown token type: {last_token['type']}\")\n",
    "    \n",
    "def token_to_pianoroll(tokens):\n",
    "    notes = []\n",
    "    frame = 0\n",
    "    last_pitch = None\n",
    "    for token in tokens:\n",
    "        if token['type'] == 'start':\n",
    "            continue\n",
    "        if token['type'] == 'pitch':\n",
    "            last_pitch = token['value']\n",
    "        if token['type'] == 'velocity':\n",
    "            notes.append(Note(onset=frame, pitch=last_pitch+21, velocity=int(token['value']*(128/32))))\n",
    "        if token['type'] == 'next_frame':\n",
    "            frame += 1\n",
    "    return PianoRoll(notes)\n",
    "    \n",
    "# logits = out[0].detach().cpu()\n",
    "\n",
    "# n_pitch = 88\n",
    "# n_velocity = 32\n",
    "# last_token = {'type':'start', 'frame':0, 'next_frame':0}\n",
    "# tokens = []\n",
    "# for frame_logits in logits:\n",
    "#     decoded = decode(frame_logits, last_token, n_pitch, n_velocity)\n",
    "#     tokens.append(decoded)\n",
    "#     last_token = decoded\n",
    "def inference(file_path:str):\n",
    "    model.eval()\n",
    "    n_pitch = 88\n",
    "    n_velocity = 32\n",
    "    tokens = [{'type':'start', 'frame':0, 'next_frame':0}]\n",
    "    #tokens = ds.tokens[64][:20]\n",
    "    last_token = tokens[-1]\n",
    "    while tokens[-1]['next_frame'] < 512:\n",
    "        input = construct_input_tensor(tokens, pos_encoding=pos_encoding, n_pitch=n_pitch, n_velocity=n_velocity).unsqueeze(0)\n",
    "        input = input.to(device)\n",
    "        logits = model(input).squeeze(0)[-1].detach().cpu()\n",
    "        decoded = decode(logits, last_token, n_pitch, n_velocity)\n",
    "        tokens.append(decoded)\n",
    "        last_token = decoded\n",
    "\n",
    "    token_to_pianoroll(tokens).to_midi(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2143 [00:00<?, ?it/s]c:\\Users\\User\\miniconda3\\envs\\gr\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      " 45%|████▍     | 956/2143 [08:58<11:08,  1.78it/s, batch=900, gpu_temp=0, loss=1.95]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     12\u001b[0m     tq \u001b[38;5;241m=\u001b[39m tqdm(dl)\n\u001b[1;32m---> 13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\gr\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\gr\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\gr\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\gr\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\gr\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 146\u001b[0m, in \u001b[0;36mTokenizedPianoRollDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    143\u001b[0m tokens_without_start \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# the last token is not needed to be an input\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_input_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_pitch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_pitch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_velocity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_velocity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m target \u001b[38;5;241m=\u001b[39m construct_target(tokens_without_start, n_pitch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_pitch, n_velocity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_velocity)\n\u001b[0;32m    148\u001b[0m output_mask \u001b[38;5;241m=\u001b[39m construct_output_mask(tokens_without_start, n_pitch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_pitch, n_velocity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_velocity)\n",
      "Cell \u001b[1;32mIn[10], line 59\u001b[0m, in \u001b[0;36mconstruct_input_tensor\u001b[1;34m(tokens, pos_encoding, n_pitch, n_velocity)\u001b[0m\n\u001b[0;32m     56\u001b[0m frame_axis \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m---> 59\u001b[0m     frame_axis\u001b[38;5;241m.\u001b[39mappend(\u001b[43mconstruct_input_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_pitch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_velocity\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(frame_axis, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m, in \u001b[0;36mconstruct_input_frame\u001b[1;34m(token, pos_encoding, n_pitch, n_velocity)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#target pos\u001b[39;00m\n\u001b[0;32m     49\u001b[0m target_pos \u001b[38;5;241m=\u001b[39m pos_encoding[token[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_frame\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpitch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvelocity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "crit.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    tq = tqdm(dl)\n",
    "    for i, batch in enumerate(tq):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        opt.zero_grad()\n",
    "        out = model(batch['input'])\n",
    "        loss = crit((out+batch['output_mask']).transpose(1,2), batch['target'])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if i % 100 == 0:\n",
    "            # print the loss to tqdm\n",
    "            #temp = torch.cuda.temperature()\n",
    "            temp = 0\n",
    "            tq.set_postfix(batch = i, loss= loss.item(), gpu_temp=temp)\n",
    "\n",
    "            if temp > 65:\n",
    "                print(\"GPU temperature is too high. Slowin down.\", temp)\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "                    \n",
    "        if torch.isnan(loss):\n",
    "            raise ValueError(\"Loss is NaN\")\n",
    "    \n",
    "    inference(f'./output_{epoch}_{i}.mid')\n",
    "    torch.save(model.state_dict(), f'./model_{epoch}.pth')\n",
    "    torch.save(opt.state_dict(), f'./opt_{epoch}.pth')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference('a.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(tq):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4])\n",
      "tensor([ 2, 36])\n",
      "tensor([5, 1])\n",
      "tensor([3, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "ds = [1,2,3,4,5,36,4,1]\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=True, num_workers=2)\n",
    "for b in dl:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
